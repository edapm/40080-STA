---
title: "40080 LC Statistics"
author: "Ed Mason"
output: html_document
---

```{r setup, include=FALSE}
library(lattice)
library(ggplot2)
library(pastecs)
library(car)
```

Last updated: `r Sys.Date()`

# Statistics Lecture 02, 30/01/2025

## Normal Distribution

```{r}
x <- seq(-4, 4, length=200)
y <- dnorm(x, mean=0, sd=1)
plot(x,y, type = "l", lwd = 2, xlim = c(-3.5,3.5))
```

## Dataset samples

```{r}
#include sample height data in data frame
height<-c(126,178,181,167,166)
gender<-c("female","female","male","female","male")
lat<-c(-43.67327,43.42426,51.79624,0.84228,-49.05650)
long<-c(-114.51627,-95.23205,-43.43058,-91.71106,-23.16819)
data<-data.frame(height,gender,lat,long)

histogram(~height|gender,data=data, main="", col="red")
```

## Plot Geodata

```{r}

```

# Statistics Lecture 03, 06/02/2025

## Hypothesis Testing

Are two samples similar or different?

### Example: Agricultural Land Use

```{r}
lowland_clay_sample <- c(10,20,25,25,30,35,45,50,50,60)
upland_limestone_sample <- c(25,30,30,40,40,50,50,60,60,65)
percentage_meadow <- data.frame(lowland_clay_sample, upland_limestone_sample)
```

Statistical hypothesis testing gives an objective method to see if something is due to chance or is genuinely statistically significant

### Six Stages

1.  State null hypothesis
2.  State alternative hypothesis
3.  Decide upon a rejection level (α)
4.  Conduct statistical test (Do the Maths)
5.  Determine whether to accept or reject the null hypothesis
6.  Draw a physical process-based conclusion from the results

#### 1. State null hypothesis

There is no significant difference between meadow percentage in lowland and upland areas. Hₒ: μ(lowland) = μ(upland)

#### 2. State alternative hypothesis

There **is** a significant difference between meadow percentage in lowland and upland areas. Hₐ: μ(lowland) != μ(upland)

##### Non-directional or Directional Tests

```{r}
# make into table
```

| Non-directional                               | Directional |
|-----------------------------------------------|-------------|
| Do NOT state which variable is greatest in Ha | State which |

variable is greatest in \| \| Cell 1, Row 2 \| Ha: μ(lowland) \> μ(upland) or μ(lowland) \< μ(upland) \|

#### 3. Decide upon a rejection level

Probability of Hₒ being true = 0.024

Probability of Hₐ being true = 0.976

The purpose of this test is to find out which hypothesis **is most likely** to be correct. (Conclusions are probabilistic not absolute).

##### Statistical significance levels

-   Usually scientists use α = 0.05 - the 95% statistical significance level (confidence level).
-   Leaves room for 5% error in our conclusion.
-   Second most commonly used is α = 0.01 - the 99% confidence level.

###### Types of Error

Type I Error - Should have retained Hₒ

Type II Error - Should have rejected Hₒ

95% level is the best level for the trade-off between Type I and Type II errors

(90% more likely Type I; 99.9% more likely Type II)

#### 4. Conduct Statistical Test

Example: Ag. Land Use Problem: Comparing means between two samples

I.e. Is there a significant difference between the two samples?

## The Student's *t* test

### Assumptions

Before you do the test, have you satisfied the assumptions?

1.  Each sample is taken from a population that is normally distributed

2.  Interval/ratio data, cannot use data in categories

3.  Each observation is independent of the other observations in the same sample

4.  The samples are taken from populations which have similar variances

#### 1. Normality

1.  Plot the data (Plot a histogram of each variable)

```{r}
# why is this splitting?
# histogram(~lowland_clay_sample|upland_limestone_sample,data=percentage_meadow,main="")
hist(percentage_meadow$lowland_clay_sample)
hist(percentage_meadow$upland_limestone_sample)
```

2.  Look at skewness values

```{r}
stat.desc(percentage_meadow$lowland_clay_sample)
stat.desc(percentage_meadow$upland_limestone_sample)
```

3.  Calculate Z score of skewness statistic
4.  The Kolmogorov-Smirnov Test (K-S Test)

#### 4. Similarity of Variances

The *t* test -\> comparing arithmetic means of two samples

So samples need an equal variance

**Rule:** Variances must differ by less than a factor of two

Formal test for this -\> Levene's Test

For the example:

```{r, include=FALSE}
# Err: Levene's test is not appropriate with quantitative explanatory variables
# leveneTest(lowland_clay_sample~upland_limestone_sample, data=percentage_meadow)
```

### Pick which form of test

One- or two-sample test (comparison to popln. mean or one another)

Independent samples or Paired samples (samples are independent of each other or are paired - measured at the same time)

$$
t = \frac{\text{difference between the means}}{\text{standard error of the difference}}
$$ Lowland Clay: Arithmetic Mean = 35% Standard Deviation = 15.81% Number of Observations = 10

Upland Limestone: Arithmetic Mean = 45% Standard Deviation = 14.14% Number of Observations = 10

# Statistics Lecture 04, 13/02/2025

## Anaylsis of Variance (ANOVA)

ANOVA determines whether the means of several populations are equal.

NB. Student's *t* test compares the means of two samples **only**.

### Two forms of ANOVA

1.  One-way ANOVA **i. Independent samples (THIS LECTURE)**

<!-- -->

ii. Paired samples (Repeated Measures ANOVA)

<!-- -->

2.  Two-way ANOVA

### One-way ANOVA: Independent Samples

-   Each observation is classified into one sample or another based on ONE criterion.

E.g. crop yield per acre obtained on different soil types

```{r}
soil_type <- c('A','A','A','A','A','B','B','B','B','B','C','C','C','C','C')
as.factor(soil_type)
crop_yield <- c(24,27,21,22,26,17,25,24,19,28,19,18,22,24,23)
table1 <- data.frame(soil_type, crop_yield)
```

ANOVA separates the total variance into two components:

1.  Variance between the samples

2.  Variance within the samples

$$
\text{Total variance} = \text{Variance between samples} + \text{Variance within samples}
\newline
\text{TSS} = \text{SSC} + \text{SSE}
$$

#### Assumptions of ANOVA

1.  The errors (residuals) are normally distributed. This is more likely to be achieved when random samples from normal populations are used. For each sample:

-   Plot a histogram
-   Calculate the skewness value
-   Calculate the Z score of skewness
-   Perform either the K-S test or the Shapiro-Wilk test

2.  The normal populations all have an equal or COMMON VARIANCE.

-   Variance ratio
-   Levene's Test

NB. If you cannot satisfy these assumptions, you need to do another test (see end of lecture).

#### Reminder: What is Variance?

$$
\text{Variance} = (\text{Standard Deviation})^2 = σ^2
$$

##### Calculating the Total Sum of Squares

The Total Sum of Squares (TSS) is the sum of the squared differences between each observation and the overall mean.

##### Calculating the Sum of Squares Between the Columns (SSC)

SSC -\> The variation between the samples

$$
\text{SSC} = \sum_{i=1}^{k} n_i(\bar{X}_i - \bar{X})^2
$$

If SSC is small, the samples have similar means.

IF SSC is large, the samples have different means.

If SSC is zero, the samples have the same mean.

##### Calculating the Error Sum of Squares (SSE)

SSE -\> The variation within the samples

$$
\text{SSE} = 
$$ High SSE means similar variations within the samples.

Low SSE means different variations (more spread) within the samples.

#### Worked Example

$$
\text{TSS} = \text{SSC} + \text{SSE}
\newline
153.6 = 19.6 + 134.0
$$

```{r}
19.6 + 134.0
```

#### ANOVA Table
Need to use Degrees of Freedom to calculate the Mean Sum of Squares

Then we can run Snedecor’s Variance Ratio Test (*F*)

$$
F = \frac{\text{Mean Sum of Squares between samples}}{\text{Mean Sum of Squares within samples}} = \frac{\text{Systematic Differences}}{\text{Random Variations}}
$$

$$
\begin{table}[]
\begin{tabular}{|c|c|lll}
\cline{1-2}
F   value        & Meaning                                              &  &  &  \\ \cline{1-2}
Less   than 1    & More   variance within samples than between samples  &  &  &  \\ \cline{1-2}
1                & Variance   within samples = Variance between samples &  &  &  \\ \cline{1-2}
Greater   than 1 & More   variance between samples than within samples  &  &  &  \\ \cline{1-2}
\end{tabular}
\end{table}
$$

#### Conclusions
1. The null hypothesis must be retained.
2. There is no significant difference in crop yields between the three soil types.
3. The differences observed between the samples are a product of random chance and do not reflect a real difference in crop yield with soil type.

### Arranging In R
Values in one column, factor in another
(see table1)

```{r}
print(table1)
res<-aov(crop_yield~soil_type, data = table1)
summary(res)
boxplot(crop_yield~soil_type, data = table1)
```
### What if Assumptions are not met?
1. Transform all of the samples (e.g. log base 10) and then test assumptions again
2. Use a non-parametric test (e.g. Kruskal-Wallis Test)

### Kruskal-Wallis Test
Only use if you CANNOT use a parametric test
```{r}
kruskal.test(crop_yield~soil_type, data = table1)
```
The bigger the value, the bigger the difference between the samples

### Post-hoc Tests
If you have more than two samples, you need to do a post-hoc test to see which samples are different from each other

#### Tukey Test
```{r}
TukeyHSD(res)
```

# Statistics Lecture 05, 20/02/2025
## The Chi-Square Test
What if your data is categorized? -> the Chi(pronounced "Kai")-Square test

### Purpose
- Assessing the similarity of two or more frequency distributions

#### Example - Expansion plans for Luton Airport
- Random sample of 100 people living in the vicinity of the airport
- Questionnaires usually begin with background questions, e.g. age,  sex, and socio-economic status (ask occupation as opposed to salary)
- Scale from 1 to 5 (1 = strongly agree, 5 = strongly disagree) -> Likert scale

<br>

- Chi-Square is all about cross-tabulation (Comparing two or more categorical variables)

(E.g. Attitudes to Airport expansion cross-tabulated by age group)

### Assumptions
1. A non-parametric test (no assumptions about distribution shape)
2. The data must be in the form of frequencies (never on continuous numerical data)
3. The samples are mutually independent (only answers one option).
4. No more than 20% of the categories should have expected frequencies of less than five (mainly to ensure sample size is large enough - will come back to later on).
5. Random and representative sampling

### Worked Example
```{r}
# create data frame
categories <- c("1 person", "2 people", "3 people", "4 or more")
exeter <- c(44, 81, 45, 30)
exmouth <- c(33, 40, 18, 9)
table2 <- data.frame(categories, exeter, exmouth)
```

#### Stage 1: Write down hypotheses
##### Null Hypothesis
The frequency distributions are identical.

##### Alternative Hypothesis
The frequency distributions are different.

#### Stage 2: Calculate expected counts
The expected counts assumed that the two frequeny distributions are identical (H₀ is true).

E.g. 1 person households
```{r}
sum_oneperson <- sum(table2$exeter[1], table2$exmouth[1])
exeter_size <- sum(table2$exeter)
exmouth_size <- sum(table2$exmouth)
total_size <- exeter_size + exmouth_size
expected_oneperson <- (sum_oneperson/total_size) * 100
expected_oneperson_exeter <- (expected_oneperson / 100) * exeter_size
expected_oneperson_exmouth <- (expected_oneperson / 100) * exmouth_size
```

Expected vs observed count

#### Stage 3: Calculate the Chi-Square Statistic
$$
\chi^2 = \frac{(O - E)^2}{E}
$$

#### Stage 4: Intepreting the Chi-Square Statistic
Chi-Square < 0 -> You have made a pig’s ear of your arithmetic.

Chi-Square = 0 -> The two frequency distributions are identical
and/or there is no association between the two variables.

As Chi-Square becomes larger and more positive -> The two frequency distributions are becoming more dissimilar (different) and/or there is a strong association between the two variables.

#### Stage 5: Compare the Chi-Square Statistic to the Critical Value
$$
\chi^2 = 5.657
$$

1. Size of table: larger tables have higher critical values
$$
\text{Degrees of freedom} = (\text{rows} - 1) * (\text{columns} - 1)
\newline
\text{df} = (4 - 1) * (2 - 1) = 3
\newline
\text{df} = 3
$$

2. Rejection level
To reject at 5% level, the critical value is 7.815

So here, we would retain the null hypothesis

#### Stage 6: Draw a conclusion
Here we have retained the null hypothesis.

Therefore, there is no difference between the distributions of household sizes in the two towns.

The distributions are similar.

The differences between the two towns are small and could have been generated by chance alone.

Note: Remember to mention at what level if you are rejecting H₀

#### Representing the data
- Clustered bar chart
```{r}
### produce clustered bar chart with table2
barplot(as.matrix(table2[,2:3]), beside = TRUE, col = c("blue", "red"), legend = rownames(table2))
```

### What if the 20% rule is violated?
- The test is only valid when no more than 20% of the cells have
EXPECTED COUNTS of less than five.
- If this rule is violated, you could aggregate data or combine categories together

### Chi-Square Test with R
```{r}
# cant get it to work
```

The formula is only an approximation (breaks down at low cell counts)