#' What we really want is a function that lets us split the data by zone in the same way that we have
#' done for the plots. To do this, we can use the function "describeBY". Conveniently, this is also in
#' the psych package. ***Search for the describeBY function in the help window and you should see the help
#' page come up with all of the arguments that are contained in the function.*** Now, run the code below.
#'
## ------------------------------------------------------------------------
describeBy(gastro$eggs, gastro$zone)
#' You will see that we have the eggs first followed by the zone. The code mat=T just tells R to keep the
#' output in a convenient matrix so that we can compare them.
#'
#' ### QUESTION
#' What is your first thought on normality? non-normal
#'
#' Let's double check our answer by running a Shapiro-Wilk test. You should know how to do this by
#' using the tapply function:
#'
## ------------------------------------------------------------------------
tapply(gastro$eggs, gastro$zone, shapiro.test)
#' Next, we need to test for homogeneity of variance. First you will need to load in the car package
#' and then add the code to test for homogeneity (i.e. Levene's test).
#'
## ------------------------------------------------------------------------
library(car)
leveneTest(eggs~zone,data=gastro)
#'
#' ### QUESTION
#' Which test should we apply to see if there is a difference in the number of eggs found between zones?
#' Conduct the test below.
#'
## ------------------------------------------------------------------------
t.test(gastro$eggs~gastro$zone,var=TRUE)
t.test
#'
#' ### QUESTION
#' Which test should we apply to see if there is a difference in the number of eggs found between zones?
#' Conduct the test below.
#'
## ------------------------------------------------------------------------
t.test(gastro$eggs~gastro$zone,var.equal=TRUE)
?leveneTest(pc~meadow, data=meadow_pc, center=mean)
rm(list=ls())
library(lattice)
library(ggplot2)
library(pastecs)
library(car)
soil_type(A,A,A,A,B,B,B,B,C,C,C,C)
soil_type <- c(A,A,A,A,B,B,B,B,C,C,C,C)
soil_type <- c('A','A','A','A','B','B','B','B','C','C','C','C')
table1 <- data.frame(soil_type)
View(table1)
summary(table1)
soil_type <- c('A','A','A','A','B','B','B','B','C','C','C','C')
as.factor(soil_type)
table1 <- data.frame(soil_type)
soil_type <- c('A','A','A','A','A','B','B','B','B','B','C','C','C','C','C')
as.factor(soil_type)
crop_yield <- c(24,27,21,22,26,17,25,24,19,28,19,18,22,24,23)
table1 <- data.frame(soil_type, crop_yield)
View(table1)
mean(~crop_yield|soil_type, data=table1)
print(table1)
res<-aov(crop_yield~soil_type, data = table1)
res
summary(res)
boxplot(crop_yield~soil_type, data = table1)
View(res)
rm(list=ls())
cd messing-with-r
dat <- PlantGrowth #this is copying the dataset into an object called 'dat'
dat #this will print the dataset to your console / screen.
##We can look at the number of datapoints in each group.For ANOVA you generally
#don't want big differences in the size of the groups (although there is not
#really a hard rule on specific size differences)
table(dat$group) #they are all equal so no issues here
#install.packages("car") #use this line if you haven't installed the package before
library(car)
leveneTest(weight ~ group, data=dat, center=mean)
res <- aov(weight ~ group, data = dat) #the data argument is for us to tell R what our dataset object is called (so it can find the data!)
res
summary(res)
r <- residuals(res)
shapiro.test(r)
hist(r)
boxplot(weight ~ group, data = dat)
boxplot(weight ~ group, data = dat, col = "blue", xlab = "Experiment groups")
library(ggplot2)
ggplot(data = dat, aes(group, weight)) + geom_boxplot() +
stat_summary(fun = mean, geom="point", shape=20, size=10, color="red", fill="red")
TUKEY <- TukeyHSD(x=res) #res is our ANOVA object from above
TUKEY
plot(TUKEY , las=1 , col="brown")
##to illustrate this test we will use a new dataset called 'airquality' (Daily
#air quality measurements in New York, May to September 1973) use the head()
#function to look at the top of the data table.
head(airquality)
leveneTest(Ozone ~ as.factor(Month), data = airquality, center = mean)
kruskal.test(Ozone ~ as.factor(Month), data = airquality)
rm(list=ls())
dat <- read.csv(file.choose())
View(dat)
dat$Diet <- as.factor(dat$Diet)
head()
head(dat)
table(dat)
table(dat$Diet)
dat$weightLost <- dat$pre.weight - dat$weight6weeks
library(car)
leveneTest()
leveneTest(weightLost~Diet, data = dat, center = mean)
boxplot(weightLost~Diet, data=dat, center=mean, main="Weight loss from different diets", col="green")
res <- aov(weightLost~Diet, data=dat)
summary(res)
r <- residuals(res)
r <- residuals(res)
shapiro.test(r)
hist(r, xlab="Residuals")
TukeyHSD(res)
plot(tukey_out)
tukey_out <- TukeyHSD(x=res)
plot(tukey_out)
#make a boxplot
boxplot(height~Diet, data=dat, center-mean, main="Height of people on different diets", col="green")
#make a boxplot
boxplot(Height~Diet, data=dat, center-mean, main="Height of people on different diets", col="green")
#make a boxplot
boxplot(Height~Diet, data=dat, center=mean, main="Height of people on different diets", col="green")
#run the anova
res2 <- aov(Height~Diet, data=dat)
#extract and inspect the residuals
r2 <- residuals(res2)
shapiro.test(r2)
hist(r2, xlab="Residuals")
#run the kruskal wallis test
kruskal.test(Height~Diet, data=dat)
rm(list=ls())
library(lattice)
library(ggplot2)
library(pastecs)
library(car)
# create data frame
table2 <- as.data.frame(matrix(c(10,20,30,40,50,60,70,80,90,100), nrow = 2, byrow = TRUE))
View(table2)
rm(list=ls())
# create data frame
categories <- c("1 person", "2 people", "3 people", "4 or more")
exeter <- c(44, 81, 45, 30)
exmouth <- c(33, 40, 18, 9)
table2 <- data.frame(categories, exeter, exmouth)
View(table2)
sum_oneperson <- sum(table2$exeter[1], table2$exmouth[1])
exeter_size <- sum(table2$exeter)
exmouth_size <- sum(table2$exmouth)
total_size <- exeter_size + exmouth_size
expected_oneperson <- (exeter_size/total_size) * sum_oneperson
expected_oneperson_exeter <- expected_oneperson * exeter_size
expected_oneperson_exmouth <- expected_oneperson * exmouth_size
expected_oneperson <- (sum_oneperson/total_size) * 100
expected_oneperson_exeter <- expected_oneperson * exeter_size
expected_oneperson_exmouth <- expected_oneperson * exmouth_size
expected_oneperson_exeter <- (expected_oneperson / 100) * exeter_size
expected_oneperson_exmouth <- (expected_oneperson / 100) * exmouth_size
barplot(as.matrix(table2[,2:3]), beside = TRUE, col = c("blue", "red"))
legend("topright", c("Exeter", "Exmouth"), fill = c("blue", "red"))
library(ggplot2)
ggplot(table2, aes(x=categories, y=exeter, fill="Exeter")) + geom_bar(stat="identity", position="dodge") + geom_bar(aes(x=categories, y=exmouth, fill="Exmouth"), stat="identity", position="dodge")
ggplot(table2, aes(x=categories, y=exeter, fill="Exeter")) + geom_bar(stat="identity", position="dodge")
### produce clustered bar chart with table2
barplot(as.matrix(table2[,2:3]), beside = TRUE, col = c("blue", "red"), legend = rownames(table2))
chisq.test(table2$exeter, table2$exmouth)
chisq.test(table2)
chisq.test(table2$exeter, table2$exmouth)
View(table2)
chisq.test(table2$categories, table2$exeter, table2$exmouth)
chisq.test(table2$exeter, table2$exmouth)
chisq.test(table2$exeter[1], table2$exmouth[1])
chisq.test(table2$exeter[1,2,3,4], table2$exmouth[1,2,3,4])
chisq.test(table2$exeter, table2$exmouth)
View(table2)
# wrong result
chisq.test(table2$exeter, table2$exmouth, correct = FALSE)
# result should be 5.636
chisq.test(sum_oneperson, p=expected_oneperson)
# result should be 5.636
chisq.test(table2[1], p=expected_oneperson)
chisq.test(table2$exeter, table2$exmouth)
table2 <- data.frame(exeter, exmouth)
chisq.test(table2$exeter, table2$exmouth)
# exeter_oneperson is 44 rows of 1
exeter_oneperson <- c(rep(1, table2$exeter[1]))
exmouth_oneperson <- c(rep(1, table2$exmouth[1]))
table3 <- data.frame(exeter_oneperson, exmouth_oneperson)
table3 <- table(exeter_oneperson, exmouth_oneperson)
setwd("~/Documents/University/Geography Degree/Year 1/Semester 2/40080 LC Statistics (STA)/Code/Week 5")
#' ---
#' title: "Week 5 Chi Square"
#' author: "Liz Hamilton"
#' date: "15/09/2020"
#' output:
#'   pdf_document: default
#'   html_document: default
#' ---
#'
## ----setup, include=FALSE------------------------------------------------
knitr::opts_chunk$set(echo = TRUE)
#'
#' ### Clearing the workspace
#'
#' It is a good idea to clear your workspace every time you start a new session in R. The
#' code below lets you do this. Also check that the global environment is clear (click the
#' little brush) and the same for plots.
#'
## ------------------------------------------------------------------------
rm(list=ls())
#' going to be go to test in most circumstances. There are two forms of the Chi Square test i.e.
#' the Goodness of Fit test (used where you only have one categorical group e.g. sex of participant)
#' and the Chi Square Test of Independence (or Association).
#'
#' Let's use the example from Ian's lecture where you looked at household size. I have put the
#' data in a file called Household.Size that you can download from Canvas and save before
#' importing. Notice that this time the file is a text file (it is easier to save in this format
#' for Chi Sq) - and that the import code has been changed accordingly.
#'
## ------------------------------------------------------------------------
Household.Size <- read.delim(file.choose(), row.names = 1)
Household.Size
#'
#' You will see that I have created a new data frame called one.person and subsetted the data.
#' This is followed by the name of the original data frame that I want to extract the data from
#' (i.e. Household.Size) and then notice, I am telling R that I want just one row and then there
#' is a comma followed by an empty space. The empty space if for specifying the number of columns.
#' In this case I want all columns so I have left it blank. There is a link here to more examples:
#'  https://dzone.com/articles/learn-r-how-extract-rows. I now have only the data I want to test
#'  (i.e. one person households) in the new data frame, one.person.
#'
## ------------------------------------------------------------------------
one.person<-Household.Size[1,]
View(Household.Size)
View(one.person)
#'
#' Let's assume that we would expect there to be an equal number of one person households in the
#' two towns. We can tell R that we expect the proportions to be i.e. 50:50 statsitically. Let's
#' create a new object called res, and then run the test. Notice that the function is chisq.test
#' and that this is followed by the data frame that you want to work with. Finally, I have told R
#' that I expect the proportions to be 50:50, or in this case 1/2 and 1/2.
#'
## ------------------------------------------------------------------------
res <- chisq.test(one.person, p = c(1/2, 1/2))
res
#' You can see that if I just look at the data for the one person households, there is not a
#' signficant difference. *However, when you did the calculations with Ian, you were not looking
#' at one person households in isolation. You were considering all of the different household
#' size categories in the survey data.* In other words, you have two groups that you want to look
#' at - the towns and the household size. So, for this we need the test of association.
#'
#' Let's work with the Household.Size file that has all of the data in it. We can conduct the
#' test as follows:
#'
## ------------------------------------------------------------------------
chisq <- chisq.test(Household.Size)
chisq
#'
#' This is similar to Ian's calculation *(although see Ian's notes at the end of the lecture for
#' why it is not exact)*. We can also see the observed and expected values by using the following
#' code:
#'
## ------------------------------------------------------------------------
chisq$observed
chisq$expected
View(chisq)
install.packages(c("airGRteaching", "cli", "curl", "dials", "jsonlite", "lintr", "parsnip", "ps", "R6", "recipes", "tinytex", "workflows", "xfun"))
install.packages(c("curl", "jsonlite", "ps", "tinytex", "workflows", "xfun"))
install.packages(c("curl", "jsonlite", "ps", "tinytex", "workflows", "xfun"))
#'
#' These do agree with Ian's values but you can see that they are to 4 d.p. rather than 2.
#'
#' Let's use another example of Ian's. You looked at the Khartoum data earlier this week. Download
#' and install the Khartoum file from Canvas and then bring it into the workspace. Take a look at
#' the structure of the data by clicking on the data frame to the right.
#'
## ------------------------------------------------------------------------
Khartoum <-read.csv(file.choose(),  header=T, sep=",")
#'
#' Notice that this is a csv file rather than a text file. As you will see, the data are in long
#' form and what we really need to do is summarise it. We can do this by putting in into a table
#' where we count all the instances in each category:
#'
## ------------------------------------------------------------------------
Khartoum.table <- with(Khartoum, table(reszone, educ))
Khartoum.table
#'
#' We can also tell R to give us the proportions:
#'
## ------------------------------------------------------------------------
Khartoum.prop<- prop.table(Khartoum.table, margin = 1)
Khartoum.prop
#'
#' For the next part. I would like you to save your file before we continue. Make sure that you
#' are saving it somewhere logical. Once you have saved it, we will set the working directory.
#' This is something that you probably have done already but not really known about it. But
#' setting the directory can make importing files etc. much easier as the browser takes you
#' straight to the file location. Go to Session > Set Working Directory > To Source File Location.
#' You will then have told R where to keep any additional files you create/install. This is
#' important for the next part where we export the table that we have created into a new Excel
#' file.
#'
write.table(Khartoum.table, file = "KhartoumNew.txt", sep = ",", quote = TRUE, row.names = T)
#' Check your working directory (i.e. where you have saved this file), you should now see a new
#' text file has appeared called KhartoumNew! This is useful if you want to save the data for
#' later analysis.
#'
#' We can also get the counts, contributions to Chi Square and summary totals using the following
#' code. This is useful for identifying those categories that make the most contribution to the
#' final Chi Square test value i.e. which categories have the most influence. The function is
#' in the gmodels package so install this first by going to Tools > Install Packages.
#'
## ------------------------------------------------------------------------
library(gmodels)
CrossTable(Khartoum$reszone, Khartoum$educ)
#'
#' Running the test is simple - we just use the function chisq.test:
#'
## ------------------------------------------------------------------------
Khartoum.chisq<-chisq.test(Khartoum$reszone, Khartoum$educ)
Khartoum.chisq
#'
#' However, you can see that there is an error message here stating that the approximation may
#' be incorrect. If we look at the expected and observed counts we will be able to see why. As
#' you know from the lecture, there are quite a few instances that fall foul of the 20% rule.
#'
## ------------------------------------------------------------------------
Khartoum.chisq$observed
Khartoum.chisq$expected
#'
#' To circumvent this, we can recode the data as you did with Ian. You will need to install the
#' plyr package first if you have not already by going to Tools > Install Packages.
#'
## ------------------------------------------------------------------------
library(plyr)
Khartoum$newEduc<- as.numeric(revalue(as.character(Khartoum$educ), c("2" = "1", "3" = "2",
"4" = "2", "5"="2")))
#'
#' This code looks a little comple but all we are doing is telling R to create a new variable
#' called newEduc and that it is numeric. We then use the revalue function and code it as a
#' character (this particular function only works for character vectors) before telling R what
#' the old value was and what should replace it. You should see that there is a new column in
#' your file with the new coded variables. We can now run the Chi Sq test again but using this
#' new variable:
#'
#'
## ------------------------------------------------------------------------
Khartoum.chisq<-chisq.test(Khartoum$reszone, Khartoum$newEduc)
Khartoum.chisq
#'
#' ### Plotting the results of Chi Sq
#'
#' As this week is rather simple in terms of the code, I thought it might be an idea to include
#' some plotting. The main plots that you will need for Chi Square are the pie charts (although
#' as and Environmental Scientist these are not my favourite!) and bar charts (ahh, much nicer
#' and less likely to make me feel slightly queasy). Despite my prejudice, there can be a time
#' and a place for a pie chart so let's do one of these first.
#'
## ------------------------------------------------------------------------
pie(Household.Size$Exeter, main="Exeter")
pie(Household.Size$Exmouth, main="Exmouth")
#'
#' This is the most simple version of a pie chart that you can create in R but you can polish it
#' to make it stand out and there are ways to make it more exciting. Let's use the Exeter data.
#'
## ------------------------------------------------------------------------
slices <- c(44, 81, 45, 30) # create a new vector with the values
lbls <- c("one", "two", "three", "four or more") # give the labels a better name
pct <- round(slices/sum(slices)*100) # convert the values to percentages
lbls <- paste(lbls, pct) # add percents to labels
lbls <- paste(lbls,"%",sep="") # ad % symbol to labels
pie(slices,labels = lbls, col=cm.colors(length(lbls)), # change the colour scheme
#- other options include rainbow, topo.colors, terrain.colors.
main="Pie Chart of Household Size in Exeter")
#'
#' My preferred option is a bar chart. For this we can use the simple bar chart standard creation,
#'  or else we can spend a bit of time and create something that is truely of publishable quality.
#'   I will show both.
#'
## ------------------------------------------------------------------------
barplot(Household.Size$Exeter, main="Exeter",
ylab="Number of Participants")
#'
#' As you can see, this is not a very good plot as I dont know what the bars relate to. This is
#' because of the way that I have set up my file Household.Size. You will see that the categories
#' are not in a separate column, so to resolve this I am going to create a new variable with the
#'  names of the categories:
#'
## ------------------------------------------------------------------------
Household.Size$Category<-c("one", "two", "three", "four or more")
#'
#' Let's plot the bar chart again with a little modification to include this:
#'
## ------------------------------------------------------------------------
barplot(Household.Size$Exeter, main="Exeter", names.arg=Household.Size$Category,
ylab="Number of Participants", xlab="Household Size", col="blue")
#'   confident. Don't worry, if you have to do it for your own data, you can arrange it in Excel
#'   and then bring in the new file.
#'
#' You will notice that the code below uses a function from the tidyr package - so install this
#' first if you haven't already. Then it uses the gather function, I have stated the old data
#' frame name and then given some new variable names of Place and Number because these are the
#' ones that I am interested in rearranging. I then tell R to rearrange the old variables Exeter
#' to Exmouth and that it is a factor.
#'
## ------------------------------------------------------------------------
library(tidyr)
Household.Size_long <- gather(Household.Size, Place, Number, Exeter:Exmouth, factor_key=TRUE)
Household.Size_long
str(Household.Size_long) # this shows that Category is a character variable
#- I want to change it to a factor variable
Household.Size_long$Category.f<-as.factor(Household.Size_long$Category)
#'
#' I can now plot the data in an easier way by using the ggplot package.
#'
## ------------------------------------------------------------------------
library(ggplot2)
ggplot(Household.Size_long,  aes(x = Category.f, y = Number, fill = Place)) +
geom_bar(position="dodge", stat="identity")
#'
#' This is a basic plot which I dont really like the look of, so to tidy it up I can make the
#' code a little more complex.
#'
## ------------------------------------------------------------------------
Household.Size_long$Category.f <- factor(Household.Size_long$Category.f,
levels = c("one", "two", "three", "four or more"))
ggplot(Household.Size_long,  aes(x = Category.f, y = Number, fill = Place)) +
geom_bar(position="dodge", stat="identity")+ # basic plot
scale_fill_manual(values = c("dark grey", "black")) + # change the bar colours
theme_bw()+ theme(axis.text = element_text(size = 12))+ # change colour scheme to black and white
theme(axis.title.y = element_text(size = 13))+ # set axis title size
xlab("") + ylab("Number of Participants") # change the titles of the x and y labels
rm(list=ls())
#' c.	Why is there a statement at the bottom of the figure regarding percentages?
#'
#' *Answers:
#' a. The five activity categories
#' b. Bar chart
#' c. To clarify that they used the raw data - not the % data to run the test. You should never run a chi square test on percentages.*
#'
#' Download the Todd data file and bring the data into R. Run the Chi Square test and see if you get a similar result as the paper (or thereabouts) - it may differ slightly as I was working with secondary plot data and it is not quite as accurate as if I had the real data.
#'
## ------------------------------------------------------------------------
todd <- read.delim("Todd-1.txt", row.names = 1, header = TRUE)
View(todd)
chisq.test(todd)
#'
#' Examine the observed and expected groups. What do the standardised residuals tell you about the effect of group size on behaviour?
#'
## ------------------------------------------------------------------------
chisq$observed
chisq <- chisq.test(todd)
#'
#' Examine the observed and expected groups. What do the standardised residuals tell you about the effect of group size on behaviour?
#'
## ------------------------------------------------------------------------
chisq$observed
chisq$expected
#'
#' Another way to look at the effect of the category on the Chi Square value is to look at the standardised residuals. Values greater than |2| when there are few cells or about 3 when there are many cells indicates lack of fit of Ho in that cell i.e. there is an important effect!
#'  We can do this with the following code:
#'
## ------------------------------------------------------------------------
chisq$stdres
#'
#' Which of the categories has the greatest effect?
#'
#' Answer: Grooming behaviour when there are no visitors is increased.
#'
#' ## Plotting the data
#'
#' The plot in Todd plots percentages. If we want to plot the percentages, we need to work them out first:
#'
## ------------------------------------------------------------------------
library(dplyr)
perc.todd = mutate(todd,
zero = Zero / sum(Zero),
small = Small / sum(Small),
large = Large / sum(Large))
perc.todd$behaviour<-c("Observing", "Feeding and chewing", "Playing", "Grooming", "Resting and sleeping")
View(chisq)
View(perc.todd)
#'
#' Notice that I have done a simple calculation to work them out first and used the mutate function in dplyr to make a new data frame called perc.todd. I will still need to rearrange this for plotting.
#'
#' Can you rearrange the data using the gather function from tidyr to create a new data frame called perc.todd_long. You should create new variables for visitor.no and percent first, and then use ggplot to graph the data.
#'
## ------------------------------------------------------------------------
library(tidyr)
?tidyr
View(perc.todd)
perc.todd_long <- gather(perc.todd, visitor.no, percent, zero:large, factor_key = TRUE)
View(perc.todd_long)
ggplot(perc.todd_long, aes(x = behaviour, y = percent, fill = visitor.no)) +
geom_bar(stat = "identity", position = "dodge") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(perc.todd_long, aes(x = behaviour, y = percent, fill = visitor.no)) +
geom_bar(stat = "identity", position = "dodge")
ggplot(perc.todd_long, aes(x = behaviour, y = percent, fill = visitor.no)) +
geom_bar(stat = "identity", position = "dodge") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(perc.todd_long, aes(x = behaviour, y = percent, fill = visitor.no, xlab="Behaviour", ylab="Fraction")) +
geom_bar(stat = "identity", position = "dodge")
library(ggplot2)
ggplot(perc.todd_long, aes(x = behaviour, y = percent, fill = visitor.no)) + geom_bar(stat = "identity", position = "dodge") + xlab("Behaviour") + ylab("Fraction")
ggplot(perc.todd_long, aes(x = behaviour, y = percent, fill = visitor.no)) + geom_bar(stat = "identity", position = "dodge") + xlab("Behaviour") + ylab("Fraction") + legend (title = "Visitor Number")
ggplot(perc.todd_long, aes(x = behaviour, y = percent, fill = visitor.no)) + geom_bar(stat = "identity", position = "dodge") + xlab("Behaviour") + ylab("Fraction") + guide.legend (title = "Visitor Number")
ggplot(perc.todd_long, aes(x = behaviour, y = percent, fill = visitor.no)) + geom_bar(stat = "identity", position = "dodge") + xlab("Behaviour") + ylab("Fraction") + guide_legend (title = "Visitor Number")
