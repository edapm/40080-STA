dataset2<-c(19, 27, 69, 17, 30, 55, 87, 32, 15, 16, 12, 21, 10, 12, 19, 18, 50, 18)
hist(dataset2)
median(dataset2)
IQR(dataset2)
## Q3
toothlen<-c(9, 10, 10, 11, 6, 7, 8, 10, 12, 13, 9, 11)
stat.desc(toothlen)
rm(list=ls())
q()
rm(list=ls())
riverwidth<-c(9.4, 5.3, 12.8, 11.6, 10.5, 15.1, 2.4, 5.7, 9.8, 11.6,
10.4, 13.1, 6.7, 9.8, 7.7, 6.7, 9.8, 8.7)
hist(riverwidth)
hist(riverwidth, col="red")
hist(riverwidth, freq = FALSE, col="red")
lines(density(riverwidth), col="blue")
river<-data.frame(riverwidth)
hist(river$riverwidth, freq = FALSE, xlab = "River Width (m)", main="")
boxplot(river$riverwidth, ylab = "River Width (m)")
bxp<-boxplot(river$riverwidth, ylab = "River Width (m)") # create the boxplot
mtext(c("Min","Max"), side=2, at=bxp$stats[c(1,5)],las=2, line=-10) # add max and min
mtext(c("Q1","Q3"), side=2, at=bxp$stats[c(2,4)], line=-8) # add quartiles
mtext(c("median"), side=2, at=bxp$stats[c(3)],las=2, line=-8) # add median
bxp<-boxplot(river$riverwidth, ylab = "River Width (m)", col="red")
install.packages("psych")
library(psych)
describe(river)
describe(river, type = 2)
se.skew = function(x){
N = length(na.omit(x))
sqrt(6*N*(N-1)/((N-2)*(N+1)*(N+3)))
}
se.skew(river$riverwidth)
Zscore<-(-0.32/0.536)
Zscore
heightweight<-read.csv(file.choose(),  header=T, sep=",")
hist(heightweight$height, freq=FALSE, col="red")
hist(heightweight$weight, freq=FALSE, col="red")
View(heightweight)
hist(heightweight$Height, col="red")
lines(density(heightweight$Height), col="blue")
hist(heightweight$Height, freq=false, col="red")
lines(density(heightweight$Height), col="blue")
hist(heightweight$Height, freq=FALSE, col="red")
lines(density(heightweight$Height), col="blue")
hist(heightweight$Weight, freq=FALSE, col="green")
lines(density(heightweight$Weight), col="yellow")
lines(density(heightweight$Weight), col="black")
ks.test(heightweight$Height, "pnorm", mean=mean(heightweight$Height),
sd=sd(heightweight$Height))
ks.test(heightweight$Weight, "pnorm", mean=mean(heightweight$Weight),
sd=sd(heightweight$Weight))
library(lattice)
histogram(~Weight|Gender,data=heightweight)
describeBy(heightweight$Weight, heightweight$Gender, mat=T)
shapiro.test(river$riverwidth)
river$flow<-c("low","low","high","high","high","high","low","low",
"low","high","high","high","low","low","low","low","low","low")
str(river)
histogram(~riverwidth|flow,data=river)
tapply(river$riverwidth, river$flow, shapiro.test)
ks.test(log(heightweight$Weight), "pnorm", mean=mean(log(heightweight$Weight)),
sd=sd(log(heightweight$Weight)))
heightweight$L_Weight<-log(heightweight$Weight)
View(heightweight)
str(heightweight)
ks.test(heightweight$L_Weight, "pnorm", mean=mean(heightweight$L_Weight),
sd=sd(heightweight$L_Weight))
hist(heightweight$Weight)
hist(heightweight$L_Weight)
library(pastecs)
stat.desc(river)
?stat.desc
stat.desc(river, p=0.99)
# It is a good idea to clear your workspace every time you start a new session in R. The
# code below lets you do this. Also check that the global environment is clear (click the
# little brush) and the same for plots if you have any.
rm(list=ls())
castor<-read.csv(file.choose(),  header=T, sep=",")
View(castor)
# ANSWER
str(castor)
castor$beaver_no<-as.factor(castor$beaver_no)
# ANSWER
str(castor)
# ANSWER
str(castor)
# ANSWER
library(lattice)
hist(~temp|beaver_no, data=castor)
hist
# ANSWER
library(lattice)
hist(~temp|beaver_no,data=castor)
# ANSWER
str(castor)
histogram(~temp|beaver_no,data=castor)
# ANSWER
histogram(~temp|active,data=castor)
# ANSWER
tapply(castor$temp, castor$beaver_no, shapiro.test)
# ANSWER
castor$log_temp<-log(castor$temp)
# ANSWER
?tapply
# QUESTION
# Plot the histogram of the log transformed data. Has log transforming helped?
histogram(~log_temp|beaver_no,data=castor)
# ANSWER
library(lattice)
# QUESTION
# Plot the histogram of the log transformed data. Has log transforming helped?
histogram(~log_temp|beaver_no,data=castor)
# ANSWER
tapply(castor$log_temp, castor$active, shapiro.test)
# QUESTION
# Plot the histogram of the log transformed data. Has log transforming helped?
histogram(~log_temp|active,data=castor)
# ANSWER
histogram(~log_temp|active,data=castor)
rm(list=ls())
library(lattice)
library(ggplot2)
lowland_clay_sample <- c(10, 20, 25, 25, 30, 35, 45, 50, 50, 60)
upland_limestone_sample <- c(25, 30, 30, 40, 40, 50, 50, 60, 60, 65)
percentage_meadow <- data.frame(lowland_clay_sample, upland_limestone_sample)
View(percentage_meadow)
histogram(~lowland_clay_sample|upland_limestone_sample,data=percentage_meadow,main="")
View(percentage_meadow)
rm(list(ls()))
rm(list=ls())
meadow_pc<-read.csv(file.choose(),  header=T, sep=",")
View(meadow_pc)
library(lattice)
library(psych)
histogram(~meadow, data=meadow_pc, main="")
histogram(~pc|meadow, data=meadow_pc, main="")
stat.desc(meadow_pc)
describe(meadow_pc)
tapply(meadow_pc$pc, meadow_pc$meadow, shapiro.test)
library(car)
leveneTest(pc~meadow, data=meadow_pc, center=mean)
t.test(meadow_pc$pc ~ meadow_pc$meadow, var.equal=TRUE)
t.test(meadow_pc$pc ~ meadow_pc$meadow, var.equal=FALSE)
t.test(meadow_pc$pc ~ meadow_pc$meadow, var.equal=TRUE, alternative = "greater")
castor<-read.csv(file.choose(),  header=T, sep=",")
View(castor)
str
str
str(castor)
castor$beaver_no<-as.factor(castor$beaver_no)
castor$active<-as.factor(castor$active)
str(castor)
describe(castor)
histogram(~temp|active, data=castor)
tapply(castor$temp, castor$beaver_no, shapiro.test)
wilcox.test(castor$temp~castor$active, paired=F)
?wilcox.test
wilcox.test(castor$temp~castor$active, paired=FALSE)
wilcox.test(castor$temp~castor$active)
AS_temp<-read.csv(file.choose(),  header=T, sep=",")
describe(AS_temp)
AS_temp$difference<-AS_temp$air_temp - AS_temp$soil_temp
hist(AS_temp$difference)
shapiro.test(AS_temp$difference)
t.test(AS_temp$air_temp, AS_temp$soil_temp, paired=TRUE)
wilcox.test(AS_temp$air_temp, AS_temp$soil_temp, paired=TRUE)
#'
#' ## Week 3: Hypothesis testing - independent and paired tests (parametric and non-parametric)
#' ## Self-test exercise
#'
#' ### Clearing the workspace
#'
#' It is a good idea to clear your workspace every time you start a new session in R. The code below lets
#' you do this. Also check that the global environment is clear (click the little brush) and the same for
#' plots if you have any.
## ------------------------------------------------------------------------
rm(list=ls())
#'
#' ### Importing a file
#' First, you will need to download the "gastro" file from Canvas and save it somewhere that you remember.
#' Import the data file:
#'
## ------------------------------------------------------------------------
gastro<-read.csv(file.choose(), header=T, separate=",")
#'
#' ### Importing a file
#' First, you will need to download the "gastro" file from Canvas and save it somewhere that you remember.
#' Import the data file:
#'
## ------------------------------------------------------------------------
gastro<-read.csv(file.choose(), header=T, sep=",")
View(gastro)
#'   differences in the number of eggs per capsule between zones. ***This is an independent comparison
#'   because individual capsules can only be in one of these two zones.***
#'
#' ### TASK
#'
#' Run some code that lets you examine the structure of the data, then run boxplots and histograms that
#' allow you to examine the number of eggs by zone.You will need the lattice package for the histogram code
#' and the psych package for the describe code.
#'
## ------------------------------------------------------------------------
str(gastro)
library(lattice)
library(psych)
describe(gastro)
View(gastro)
histogram(~eggs|zone,data=gastro, main="")
boxplot(gastro$eggs~gastro$zone,ylab="Number of Eggs",xlab="Zone")
#' What we really want is a function that lets us split the data by zone in the same way that we have
#' done for the plots. To do this, we can use the function "describeBY". Conveniently, this is also in
#' the psych package. ***Search for the describeBY function in the help window and you should see the help
#' page come up with all of the arguments that are contained in the function.*** Now, run the code below.
#'
## ------------------------------------------------------------------------
describeBy(gastro$eggs, gastro$zone)
#' You will see that we have the eggs first followed by the zone. The code mat=T just tells R to keep the
#' output in a convenient matrix so that we can compare them.
#'
#' ### QUESTION
#' What is your first thought on normality? non-normal
#'
#' Let's double check our answer by running a Shapiro-Wilk test. You should know how to do this by
#' using the tapply function:
#'
## ------------------------------------------------------------------------
tapply(gastro$eggs, gastro$zone, shapiro.test)
#' Next, we need to test for homogeneity of variance. First you will need to load in the car package
#' and then add the code to test for homogeneity (i.e. Levene's test).
#'
## ------------------------------------------------------------------------
library(car)
leveneTest(eggs~zone,data=gastro)
#'
#' ### QUESTION
#' Which test should we apply to see if there is a difference in the number of eggs found between zones?
#' Conduct the test below.
#'
## ------------------------------------------------------------------------
t.test(gastro$eggs~gastro$zone,var=TRUE)
t.test
#'
#' ### QUESTION
#' Which test should we apply to see if there is a difference in the number of eggs found between zones?
#' Conduct the test below.
#'
## ------------------------------------------------------------------------
t.test(gastro$eggs~gastro$zone,var.equal=TRUE)
?leveneTest(pc~meadow, data=meadow_pc, center=mean)
rm(list=ls())
library(lattice)
library(ggplot2)
library(pastecs)
library(car)
soil_type(A,A,A,A,B,B,B,B,C,C,C,C)
soil_type <- c(A,A,A,A,B,B,B,B,C,C,C,C)
soil_type <- c('A','A','A','A','B','B','B','B','C','C','C','C')
table1 <- data.frame(soil_type)
View(table1)
summary(table1)
soil_type <- c('A','A','A','A','B','B','B','B','C','C','C','C')
as.factor(soil_type)
table1 <- data.frame(soil_type)
soil_type <- c('A','A','A','A','A','B','B','B','B','B','C','C','C','C','C')
as.factor(soil_type)
crop_yield <- c(24,27,21,22,26,17,25,24,19,28,19,18,22,24,23)
table1 <- data.frame(soil_type, crop_yield)
View(table1)
mean(~crop_yield|soil_type, data=table1)
print(table1)
res<-aov(crop_yield~soil_type, data = table1)
res
summary(res)
boxplot(crop_yield~soil_type, data = table1)
View(res)
rm(list=ls())
cd messing-with-r
dat <- PlantGrowth #this is copying the dataset into an object called 'dat'
dat #this will print the dataset to your console / screen.
##We can look at the number of datapoints in each group.For ANOVA you generally
#don't want big differences in the size of the groups (although there is not
#really a hard rule on specific size differences)
table(dat$group) #they are all equal so no issues here
#install.packages("car") #use this line if you haven't installed the package before
library(car)
leveneTest(weight ~ group, data=dat, center=mean)
res <- aov(weight ~ group, data = dat) #the data argument is for us to tell R what our dataset object is called (so it can find the data!)
res
summary(res)
r <- residuals(res)
shapiro.test(r)
hist(r)
boxplot(weight ~ group, data = dat)
boxplot(weight ~ group, data = dat, col = "blue", xlab = "Experiment groups")
library(ggplot2)
ggplot(data = dat, aes(group, weight)) + geom_boxplot() +
stat_summary(fun = mean, geom="point", shape=20, size=10, color="red", fill="red")
TUKEY <- TukeyHSD(x=res) #res is our ANOVA object from above
TUKEY
plot(TUKEY , las=1 , col="brown")
##to illustrate this test we will use a new dataset called 'airquality' (Daily
#air quality measurements in New York, May to September 1973) use the head()
#function to look at the top of the data table.
head(airquality)
leveneTest(Ozone ~ as.factor(Month), data = airquality, center = mean)
kruskal.test(Ozone ~ as.factor(Month), data = airquality)
rm(list=ls())
dat <- read.csv(file.choose())
View(dat)
dat$Diet <- as.factor(dat$Diet)
head()
head(dat)
table(dat)
table(dat$Diet)
dat$weightLost <- dat$pre.weight - dat$weight6weeks
library(car)
leveneTest()
leveneTest(weightLost~Diet, data = dat, center = mean)
boxplot(weightLost~Diet, data=dat, center=mean, main="Weight loss from different diets", col="green")
res <- aov(weightLost~Diet, data=dat)
summary(res)
r <- residuals(res)
r <- residuals(res)
shapiro.test(r)
hist(r, xlab="Residuals")
TukeyHSD(res)
plot(tukey_out)
tukey_out <- TukeyHSD(x=res)
plot(tukey_out)
#make a boxplot
boxplot(height~Diet, data=dat, center-mean, main="Height of people on different diets", col="green")
#make a boxplot
boxplot(Height~Diet, data=dat, center-mean, main="Height of people on different diets", col="green")
#make a boxplot
boxplot(Height~Diet, data=dat, center=mean, main="Height of people on different diets", col="green")
#run the anova
res2 <- aov(Height~Diet, data=dat)
#extract and inspect the residuals
r2 <- residuals(res2)
shapiro.test(r2)
hist(r2, xlab="Residuals")
#run the kruskal wallis test
kruskal.test(Height~Diet, data=dat)
rm(list=ls())
library(lattice)
library(ggplot2)
library(pastecs)
library(car)
# create data frame
table2 <- as.data.frame(matrix(c(10,20,30,40,50,60,70,80,90,100), nrow = 2, byrow = TRUE))
View(table2)
rm(list=ls())
# create data frame
categories <- c("1 person", "2 people", "3 people", "4 or more")
exeter <- c(44, 81, 45, 30)
exmouth <- c(33, 40, 18, 9)
table2 <- data.frame(categories, exeter, exmouth)
View(table2)
sum_oneperson <- sum(table2$exeter[1], table2$exmouth[1])
exeter_size <- sum(table2$exeter)
exmouth_size <- sum(table2$exmouth)
total_size <- exeter_size + exmouth_size
expected_oneperson <- (exeter_size/total_size) * sum_oneperson
expected_oneperson_exeter <- expected_oneperson * exeter_size
expected_oneperson_exmouth <- expected_oneperson * exmouth_size
expected_oneperson <- (sum_oneperson/total_size) * 100
expected_oneperson_exeter <- expected_oneperson * exeter_size
expected_oneperson_exmouth <- expected_oneperson * exmouth_size
expected_oneperson_exeter <- (expected_oneperson / 100) * exeter_size
expected_oneperson_exmouth <- (expected_oneperson / 100) * exmouth_size
barplot(as.matrix(table2[,2:3]), beside = TRUE, col = c("blue", "red"))
legend("topright", c("Exeter", "Exmouth"), fill = c("blue", "red"))
library(ggplot2)
ggplot(table2, aes(x=categories, y=exeter, fill="Exeter")) + geom_bar(stat="identity", position="dodge") + geom_bar(aes(x=categories, y=exmouth, fill="Exmouth"), stat="identity", position="dodge")
ggplot(table2, aes(x=categories, y=exeter, fill="Exeter")) + geom_bar(stat="identity", position="dodge")
### produce clustered bar chart with table2
barplot(as.matrix(table2[,2:3]), beside = TRUE, col = c("blue", "red"), legend = rownames(table2))
chisq.test(table2$exeter, table2$exmouth)
chisq.test(table2)
chisq.test(table2$exeter, table2$exmouth)
View(table2)
chisq.test(table2$categories, table2$exeter, table2$exmouth)
chisq.test(table2$exeter, table2$exmouth)
chisq.test(table2$exeter[1], table2$exmouth[1])
chisq.test(table2$exeter[1,2,3,4], table2$exmouth[1,2,3,4])
chisq.test(table2$exeter, table2$exmouth)
View(table2)
# wrong result
chisq.test(table2$exeter, table2$exmouth, correct = FALSE)
# result should be 5.636
chisq.test(sum_oneperson, p=expected_oneperson)
# result should be 5.636
chisq.test(table2[1], p=expected_oneperson)
chisq.test(table2$exeter, table2$exmouth)
table2 <- data.frame(exeter, exmouth)
chisq.test(table2$exeter, table2$exmouth)
# exeter_oneperson is 44 rows of 1
exeter_oneperson <- c(rep(1, table2$exeter[1]))
exmouth_oneperson <- c(rep(1, table2$exmouth[1]))
table3 <- data.frame(exeter_oneperson, exmouth_oneperson)
table3 <- table(exeter_oneperson, exmouth_oneperson)
rm(list=ls())
library(lattice)
library(ggplot2)
library(pastecs)
library(car)
r
# Clear your workspace as usual:
rm(list=ls())
setwd("~/Documents/University/Geography Degree/Year 1/Semester 2/40080 LC Statistics (STA)/Code/Week 6")
# Listed below are the pacakges that you will need for the code this week. If you have not installed
# them already, please do so by navigating via the Tools tab at the top before you run the library
# code.
library(tidyr)
library(Hmisc)
library(corrplot)
library(PerformanceAnalytics)
# Bring in the data and examine the structure:
nh4no3<-read.csv(file.choose(),  header=T, sep=",")
str(nh4no3)
View(nh4no3)
# Examine the data by looking a the plots and normality. You can change the variables if
# you want to examine them all...but we will work through with nitrate (NO3) and ammonia (nh4) first.
boxplot(nh4no3$no3, ylab="Nitrate (mg/kg)")
boxplot(nh4no3$nh4, ylab="Ammonia (mg/kg)")
library(lattice)
histogram(nh4no3$no3, type="density")
histogram(nh4no3$nh4, type="density")
library(psych)
describe(nh4no3$no3)
describe(nh4no3$nh4)
# Run a test for normality - we will use the Shapiro-Wilk test because we have < 2000 samples
shapiro.test(nh4no3$no3)
shapiro.test(nh4no3$nh4)
# Correlation starts with a plot. We might have good reason to think that ammonia and nitrate
# in soils are related (i.e. nitrification and mineralisation) - it is not important to know the
# reason for now, but they are both forms of soil nitrogen.
plot(nh4no3$nh4,nh4no3$no3)
# In Ian's lectures, you calculated the correlation coefficient r. We can do this in R using the
# following code:
res<- cor(nh4no3, method = "pearson")
res
# To get the full output for a correlation, we can use the following code:
res1<-cor.test(nh4no3$nh4, nh4no3$no3, method = "pearson")
res1
# The output gives the p value and the correlation coefficient (see Ian's notes for further
# explanation).
# Notice that in this case, I have indicated that the method that I want to use is Pearson's.
# This is because I know that my data are normal. If my data were not normal, I could run the
# following code:
cor.test(nh4no3$nh4, nh4no3$no3, method = "spearman")
# HOwever, I get an error message saying that R cannot compute exact p values with ties. This
# is because Spearman's test considers the rank of values and in this case, there are two or
# move values with the same rank. We can get around this with a small addition:
res2<-cor.test(nh4no3$nh4, nh4no3$no3, method = "spearman", exact=F)
res2
# If we wanted to examine the relationship between multiple variables in one go, we could run the
# following code which give us just the correlation coeffcient for all of the variables in our
# data frame. The code round( , 3) just tells R to limit the number of decimal places to 3.
res3<-round(cor(nh4no3, method = "pearson", use = "complete.obs"),3)
res3
# The function rcorr() [in Hmisc package] can be used to compute the significance levels for pearson and
# spearman correlations. It returns both the correlation coefficients and the p-value of the correlation for all
# possible pairs of columns in the data table.
library(Hmisc)
res4<-rcorr(as.matrix(nh4no3),type=c("spearman"))
res4
# we can also extract specific aspect from the res4 object:
res4$r # Extracts the r values
res4$P # Extracts p-values
# Sometimes it is easier to visualise correlations. The corrplot function is one code that can
# be used but please note it only works with the cor function, not the cor.test function.
library(corrplot)
corrplot(res, type = "upper", method = "number")
# My favourite visualisation though is the one in the PerformanceAnalytics package. Although
# I appreciate that not everyone will be a fan:
library("PerformanceAnalytics")
chart.Correlation(nh4no3, histogram=TRUE, pch=19)
# Clear your workspace as usual:
rm(list=ls())
# TASK 1: Bring in the data set called GlobalTempCo2 which you should download from Canvas. Then look at
# the structure of the data. The data are Global Annual Temperature Anomalies on Land and Annual
# Global Average CO2 for 1959 - 2012.
read.csv("GlobalTempCo2.csv", header = TRUE)
# TASK 1: Bring in the data set called GlobalTempCo2 which you should download from Canvas. Then look at
# the structure of the data. The data are Global Annual Temperature Anomalies on Land and Annual
# Global Average CO2 for 1959 - 2012.
GlobalTempCo2 <- read.csv("GlobalTempCo2.csv", header = TRUE)
View(GlobalTempCo2)
library(tidyr)
library(Hmisc)
library(corrplot)
library(PerformanceAnalytics)
View(GlobalTempCo2)
# TASK 2: Examine the data by looking at plots and normality
plot(GlobalTempCo2$Land, GlobalTempCo2$CO2)
# What do the plots say about normality?
boxplot(GlobalTempCo2$Land)
boxplot(GlobalTempCo2$CO2)
histogram(GlobalTempCo2$Land)
histogram(GlobalTempCo2$CO2)
describe(GlobalTempCo2$Land)
describe(GlobalTempCo2$CO2)
# TASK 3: Run a test for normality as well.
shapiro.test(GlobalTempCo2$Land)
shapiro.test(GlobalTempCo2$CO2)
# TASK 4: Plot the data and visualise the correlations before running an appropriate test.
plot(GlobalTempCo2$Land, GlobalTempCo2$CO2)
res<-cor.test(GlobalTempCo2$Land, GlobalTempCo2$CO2, method="spearman", exact=F)
res
rm(list=ls())
